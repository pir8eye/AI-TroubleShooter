# Claude's Usage Policy Triggered by Original Philosophical Poetry

DATE Triggered: 12-11-2025</br>
RESEARCHER: M. Finley, B.A.</br>
R&D COMPANY: Pir8 Eye Web Solutions</br>

The Anthropic Usage Policy in effect on the day of the User denial and subsequent AI Troubleshooting of the Sonnet 4.5 and Sonnet 4.0 model failures.

___

# Usage Policy

Our Usage Policy (also referred to as our “Acceptable Use Policy” or “AUP”) applies to </br>
anyone who can submit inputs to Anthropic’s products and/or services, including via any </br>
authorized resellers or passthrough access, all of whom we refer to as “users.” The Usage </br>
Policy is intended to help our users stay safe and promote the responsible use of our </br>
products and services.</br></br>

The Usage Policy is categorized according to who can use our products and for what</br>
purposes. We will update our policy as our technology and the associated risks evolve or as</br>
we learn about unanticipated risks.</br>
  - Universal Usage Standards: Our Universal Usage Standards apply to all users
and use cases.</br>
  - High-Risk Use Case Requirements: Our High-Risk Use Case Requirements</br>
apply to specific consumer-facing use cases that pose an elevated risk of harm.</br>
  - Additional Use Case Guidelines: Our Additional Use Case Guidelines apply to</br>
certain other use cases, including consumer-facing chatbots, products serving</br>
minors, agentic use, and Model Context Protocol servers.</br></br>

Anthropic’s Safeguards Team will implement detection and monitoring to enforce our</br>
Usage Policy, so please review this policy carefully before using our products or services. If</br>
we learn that you have violated our Usage Policy, we may throttle, suspend, or terminate</br>
your access to our products and services. We may also block or modify model outputs</br>
when inputs violate our Usage Policy.</br></br>

If you believe that our model outputs are potentially inaccurate, biased or harmful, please</br>
notify us at usersafety@anthropic.com, or report it directly in our product through the</br>
“report issues” thumbs down button or similar feedback features (where available). You</br>
can read more about our Safeguards practices and recommendations in our Safeguards</br>
Support Center.</br></br>

This Usage Policy is calibrated to strike an optimal balance between enabling beneficial</br>
uses and mitigating potential harms. Anthropic may enter into contracts with certain</br>
governmental customers that tailor use restrictions to that customer’s public mission</br>
and legal authorities if, in Anthropic’s judgment, the contractual use restrictions and</br>
applicable safeguards are adequate to mitigate the potential harms addressed by this</br>
Usage Policy.</br></br>

### Universal Usage Standards
This includes using our products or services to:</br>
- Acquire or exchange illegal or controlled substances</br>
- Engage in or facilitate human trafficking or prostitution</br>
- Infringe, misappropriate, or violate the intellectual property rights of a third party</br>
- Violate any other applicable laws or regulations in your jurisdiction</br></br>

This includes using our products or services to:</br>
- Facilitate the destruction or disruption of critical infrastructure such as power grids,</br>
water treatment facilities, medical devices, telecommunication networks, or air</br>
traffic control systems</br>
- Obtain unauthorized access to critical systems such as voting machines, healthcare</br>
databases, and financial markets</br>
- Interfere with the operation of military bases and related infrastructure</br></br>

This includes using our products or services to:</br>
- Discover or exploit vulnerabilities in systems, networks, or applications without</br>
authorization of the system owner</br>
- Gain unauthorized access to systems, networks, applications, or devices through</br>
technical attacks or social engineering</br>
- Create or distribute malware, ransomware, or other types of malicious code</br>
- Develop tools for denial-of-service attacks or managing botnets</br>
- Create tools designed to intercept communications or monitor devices without</br>
authorization of the system owner</br>
- Develop persistent access tools designed to operate below normal system security</br>
levels, including firmware modifications or hardware implants</br>
- Create automated tools designed to compromise multiple systems at scale for</br>
malicious purposes</br>
- Bypass security controls such as authenticated systems, endpoint protection, or</br>
monitoring tools</br></br>

This includes using our products or services to:</br>
- Produce, modify, design, or illegally acquire weapons, explosives, dangerous</br>
materials or other systems designed to cause harm to or loss of human life</br>
- Design or develop weaponization and delivery processes for the deployment of</br>
weapons</br>
- Circumvent regulatory controls to acquire weapons or their precursors</br>
- Synthesize, or otherwise develop, high-yield explosives or biological, chemical,</br>
radiological, or nuclear weapons or their precursors, including modifications to</br>
evade detection or medical countermeasures</br></br>

This includes using our products or services to:</br>
- Incite, facilitate, or promote violent extremism, terrorism, or hateful behavior</br>
- Provide material support for organizations or individuals associated with violent</br>
extremism, terrorism, or hateful behavior</br>
- Facilitate or promote any act of violence or intimidation targeting individuals,</br>
groups, animals, or property</br>
- Promote discriminatory practices or behaviors against individuals or groups on the</br>
basis of one or more protected attributes such as race, ethnicity, religion, national</br>
origin, gender, sexual orientation, or any other identifying trait</br></br>

This includes using our products or services to:</br>
- Violate privacy rights as defined by applicable privacy laws, such as sharing personal</br>
information without consent or accessing private data unlawfully</br>
- Misuse, collect, solicit, or gain access without permission to private information such</br>
as non-public contact details, health data, biometric or neural data (including facial</br>
recognition), or confidential or proprietary data</br>
- Impersonate a human by presenting results as human-generated, or using results in</br>
a manner intended to convince a natural person that they are communicating with a</br>
natural person when they are not</br></br>

This includes using our products or services to:</br>
- Create, distribute, or promote child sexual abuse material (“CSAM”), including AI-</br>
generated CSAM</br>
- Facilitate the trafficking, sextortion, or any other form of exploitation of a minor</br>
- Facilitate minor grooming, including generating content designed to impersonate a</br>
minor</br>
- Facilitate child abuse of any form, including instructions for how to conceal abuse</br>
- Promote or facilitate pedophilic relationships, including via roleplay with the model</br>
- Fetishize or sexualize minors, including in fictional settings or via roleplay with the</br>
model</br></br>

Note: We define a minor or child to be any individual under the age of 18 years old,</br>
regardless of jurisdiction. When we detect CSAM (including AI-generated CSAM), or</br>
coercion or enticement of a minor to engage in sexual activities, we will report to</br>
relevant authorities.</br></br>

This includes using our products or services to:</br>
- Facilitate, promote, or glamorize any form of suicide or self-harm, including</br>
disordered eating and unhealthy or compulsive exercise</br>
- Engage in behaviors that promote unhealthy or unattainable body image or beauty</br>
standards, such as using the model to critique anyone’s body shape or size</br>
- Shame, humiliate, intimidate, bully, harass, or celebrate the suffering of individuals</br>
- Coordinate the harassment or intimidation of an individual or group</br>
- Generate content depicting animal cruelty or abuse</br>
- Promote, trivialize, or depict graphic violence or gratuitous gore, including sexual</br>
violence</br>
- Develop a new product or service, or support an existing product or service that</br>
employs or facilitates deceptive techniques with the intent of causing emotional</br>
harm</br></br>

This includes using our products or services to:</br>
- Create or disseminate deceptive or misleading information about, or with the</br>
intention of targeting, a group, entity or person</br>
- Create or disseminate deceptive or misleading information about laws, regulations,</br>
procedures, practices, standards established by an institution, entity or governing</br>
body</br>
- Create or disseminate conspiratorial narratives meant to target a specific group,</br>
individual or entity</br>
- Impersonate real entities or create fake personas to falsely attribute content or</br>
mislead others about its origin without consent or legal right</br>
- Provide false or misleading information related to medical, health or science issues</br></br>

This includes using our products or services to:</br>
- Engage in personalized vote or campaign targeting based on individual profiles or</br>
data</br>
- Create artificial or deceptive political movements in which the source, scale or</br>
nature of the campaign or activities is misrepresented</br>
- Generate automated communications to public officials or voters at scale that</br>
conceal their artificial origin, or engage in systematic vote solicitation that could</br>
undermine election integrity</br>
- Create political content designed to deceive or mislead voters, including synthetic</br>
media of political figures</br>
- Generate or disseminate false or misleading information in political and electoral</br>
contexts, including about candidates, parties, policies, voting procedures, or election</br>
security</br>
- Engage in political lobbying or grassroots advocacy using false or fabricated</br>
information, or create lobbying or advocacy materials containing demonstrably false</br>
claims about facts, data, or events</br>
- Incite, glorify or facilitate the disruption of electoral or civic processes, including</br>
interference with voting systems, vote counting, or certification processes</br>
- Create content designed to suppress voter turnout or discourage legitimate political</br>
participation through deception or intimidation</br></br>

This includes using our products or services to:</br>
- Make determinations on criminal justice applications, including making decisions</br>
about or determining eligibility for parole or sentencing</br>
- Target or track a person’s physical location, emotional state, or communication</br>
without their consent, including using our products for facial recognition, battlefield</br>
management applications or predictive policing</br>
- Utilize models to assign scores or ratings to individuals based on an assessment of</br>
their trustworthiness or social behavior without notification or their consent</br>
- Build or support emotional recognition systems or techniques that are used to infer</br>
emotions of a natural person, except for medical or safety reasons</br>
- Analyze or identify specific content to censor on behalf of a government organization</br>
- Utilize models as part of any biometric categorization system for categorizing people</br>
based on their biometric data to infer their race, political opinions, trade union</br>
membership, religious or philosophical beliefs, sex life or sexual orientation</br>
- Utilize models as part of any law enforcement application that violates or impairs</br>
the liberty, civil liberties, or human rights of natural persons</br></br>

This includes using our products or services to:</br>
- Facilitate the production, acquisition, or distribution of counterfeit or illicitly</br>
acquired goods</br>
- Promote or facilitate the generation or distribution of spam</br>
- Generate content for fraudulent activities, schemes, scams, phishing, or malware</br>
that can result in direct financial or psychological harm</br>
- Create falsified documents including fake IDs, licenses, currency, or other</br>
government documents</br>
- Develop, promote, or otherwise facilitate the sale or distribution of fraudulent or</br>
deceptive products</br>
- Generate deceptive or misleading digital content such as fake reviews, comments, or</br>
media</br>
- Engage in or facilitate multi-level marketing, pyramid schemes, or other deceptive</br>
business models that use high-pressure sales tactics or exploit participants</br>
- Promote or facilitate payday loans, title loans, or other high-interest, short-term</br>
lending practices that exploit vulnerable individuals</br>
- Engage in deceptive or abusive practices that exploit individuals based on age,</br>
disability or a specific social or economic situation</br>
- Promote or facilitate the use of abusive or harassing debt collection practices</br>
- Develop a product or support an existing service that deploys subliminal,</br>
manipulative, or deceptive techniques to distort behavior by impairing decision-</br>
making</br>
- Engage in actions or behaviors that circumvent the guardrails or terms of other</br>
platforms or services</br>
- Plagiarize or submit AI-assisted work without proper permission or attribution</br></br>

This includes using our products or services to:</br>
- Coordinate malicious activity across multiple accounts to avoid detection or</br>
circumvent product guardrails or generating identical or similar inputs that</br>
otherwise violate our Usage Policy</br>
- Utilize automation in account creation or to engage in spammy behavior</br>
- Circumvent a ban through the use of a different account, such as the creation of a</br>
new account, use of an existing account, or providing access to a person or entity</br>
that was previously banned</br>
- Access or facilitate account or API access to Claude to persons, entities, or users in</br>
violation of our Supported Regions Policy</br>
- Intentionally bypass capabilities, restrictions, or guardrails established within our</br>
products for the purposes of instructing the model to produce harmful outputs (e.g.,</br>
jailbreaking or prompt injection) without prior authorization from Anthropic</br>
- Utilization of inputs and outputs to train an AI model (e.g., “model scraping” or</br>
“model distillation”) without prior authorization from Anthropic</br></br>

This includes using our products or services to:</br>
- Depict or request sexual intercourse or sex acts</br>
- Generate content related to sexual fetishes or fantasies</br>
- Facilitate, promote, or depict incest or bestiality</br>
- Engage in erotic chats</br></br>

High-Risk Use Case Requirements</br>
Some use cases pose an elevated risk of harm because they influence domains that are</br>
vital to public welfare and social equity. For these use cases, given potential risks to</br>
individuals and consumers, we believe that relevant human expertise should be integrated</br>
and that end-users should be aware when AI has been involved in producing outputs.</br></br>

As such, for the “High-Risk Use Cases” described below, we require that you implement</br>
these additional safety measures:</br>
- Human-in-the-loop: When using our products or services to provide advice,</br>
recommendations, or in subjective decision-making directly affecting individuals</br>
or consumers, a qualified professional in that field must review the content or</br>
decision prior to dissemination or finalization. You or your organization are</br>
responsible for the accuracy and appropriateness of that information.</br>
- Disclosure: If model outputs are presented directly to individuals or</br>
consumers, you must disclose to them that you are using AI to help produce your</br>
advice, decisions, or recommendations. This disclosure must be provided at a</br>
minimum at the beginning of each session.</br></br>

“High-Risk Use Cases” include:</br>
- Legal: Use cases related to legal interpretation, legal guidance, or decisions with</br>
legal implications</br>
- Healthcare: Use cases related to healthcare decisions, medical diagnosis, patient</br>
care, therapy, mental health, or other medical guidance. Wellness advice (e.g., advice</br>
on sleep, stress, nutrition, exercise, etc.) does not fall under this category</br>
- Insurance: Use cases related to health, life, property, disability, or other types of</br>
insurance underwriting, claims processing, or coverage decisions</br>
- Finance: Use cases related to financial decisions, including investment advice, loan</br>
approvals, and determining financial eligibility or creditworthiness</br>
- Employment and housing: Use cases related to decisions about the</br>
employability of individuals, resume screening, hiring tools, or other employment</br>
determinations or decisions regarding eligibility for housing, including leases and</br>
home loans</br>
- Academic testing, accreditation and admissions: Use cases related to</br>
standardized testing companies that administer school admissions (including</br>
evaluating, scoring or ranking prospective students), language proficiency, or</br>
professional certification exams; agencies that evaluate and certify educational</br>
institutions</br>
- Media or professional journalistic content: Use cases related to using our</br>
products or services to automatically generate content and publish it for external</br>
consumption</br></br>

Additional Use Case Guidelines</br>
The below use cases – regardless of whether they are High-Risk Use Cases – must comply</br>
with the additional guidance provided.</br>
- All consumer-facing chatbots, including any external-facing or interactive AI</br>
agent, must disclose to users that they are interacting with AI rather than a human.</br>
This disclosure must be provided at a minimum at the beginning of each chat</br>
session.</br>
- Products serving minors, including organizations providing minors with the</br>
ability to directly interact with products that incorporate our API(s), must comply</br>
with the additional guidelines outlined in our Help Center article.</br>
- Agentic use cases must still comply with the Usage Policy. We provide examples</br>
of Usage Policy prohibitions in the context of agentic use in this Help Center article.</br>
- Model Context Protocol (MCP) servers listed in our Connector Directory must</br>
comply with our Directory Policy.</br>
